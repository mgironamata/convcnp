{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DWJXd8T78uHQ"
   },
   "source": [
    "# Implementing and Training Convolutional Conditional Neural Processes\n",
    "\n",
    "The ConvCNP ([Convolutional Conditional Neural Process](https://openreview.net/forum?id=Skey4eBYPS)) is a recently introduced member of the neural process family that leverages translation equivariance as an inductive bias when performing inference on sets. This repository contains the  necessary classes and scripts to reproduce the 1d Gaussian process experiments from the ICLR paper. \n",
    "\n",
    "In this notebook, we will take a more illustrative look at how the ConvCNP is implemented, trained, and performs on this toy data. Let us begin by importing the necessary packages, as well as pieces from the current repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1222,
     "status": "ok",
     "timestamp": 1588078527368,
     "user": {
      "displayName": "M. Girona-Mata",
      "photoUrl": "",
      "userId": "05153816392464611766"
     },
     "user_tz": -60
    },
    "id": "oZdCxl_-8w_t",
    "outputId": "81f912ba-b47a-4565-cdf2-5af69e767027"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\marcg\\\\Google Drive\\\\MResProject\\\\convcnp\\\\convcnp'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "#%cd convcnp\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5273,
     "status": "ok",
     "timestamp": 1588078531443,
     "user": {
      "displayName": "M. Girona-Mata",
      "photoUrl": "",
      "userId": "05153816392464611766"
     },
     "user_tz": -60
    },
    "id": "_7MkRe_m9Ppf",
    "outputId": "d4564024-cac8-4c74-f532-703c8ce2881c"
   },
   "outputs": [],
   "source": [
    "#os.chdir('c:\\\\Users\\\\marcg\\\\Google Drive\\\\MResProject')\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install pytorch torchvision cudatoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_RgZgmbk8uHT",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import stheno.torch as stheno\n",
    "\n",
    "#import convcnp.data\n",
    "import convcnp.data_hydro_2\n",
    "\n",
    "from convcnp.experiment import report_loss, RunningAverage\n",
    "from convcnp.utils import gaussian_logpdf, init_sequential_weights, to_multiple\n",
    "from convcnp.architectures import SimpleConv, UNet\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "def to_numpy(x):\n",
    "    \"\"\"Convert a PyTorch tensor to NumPy.\"\"\"\n",
    "    return x.squeeze().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 472 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "filepath = r'../../data/camels/basin_timeseries_v1p2_modelOutput_maurer/model_output_maurer/model_output/flow_timeseries/maurer/01/01013500_05_model_output.txt'\n",
    "df = pd.read_table(filepath, sep=\"\\s+\")\n",
    "\n",
    "def date_to_int(row):\n",
    "    return int(time.mktime(datetime.datetime(year=int(row['YR']), month=int(row['MNTH']), day=int(row['DY'])).timetuple())/86400)\n",
    "\n",
    "df['idx'] = df.apply(date_to_int,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dewWREGN8uHY"
   },
   "source": [
    "## Data Generation\n",
    "\n",
    "As other members of the NP family, ConvCNP learns from a large collection of related tasks, where each training task contains a _context set_ and a _target set_. The context set is used to infer the predictive function, which is evaluated on the target set. In this experiment we will use a Gaussian process to sample a function, from which we will sub-sample (distinct) context and target sets. \n",
    "\n",
    "In this repository, we use [Stheno](https://github.com/wesselb/stheno) to handle GPs. Our generator class (found in [convnp.data](https://github.com/cambridge-mlg/convcnp/blob/master/convcnp/data.py)) handles the data generation process for us! All we need to do is create a Stheno kernel and pass it to the generator (other arguments like batch size and number of tasks per \"epoch\" can also be controlled).\n",
    "\n",
    "Let us define such a generator, and plot a few tasks to see what these look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22584,
     "status": "ok",
     "timestamp": 1588078548797,
     "user": {
      "displayName": "M. Girona-Mata",
      "photoUrl": "",
      "userId": "05153816392464611766"
     },
     "user_tz": -60
    },
    "id": "E4sS9Ood8uHZ",
    "outputId": "e5365e51-135f-4fb1-feb2-3b55462d73bf"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#kernel = stheno.Matern52().stretch(0.25)\n",
    "#gen = convcnp.data.GPGenerator(kernel=kernel)\n",
    "gen = convcnp.data_hydro_2.HydroGenerator(s_year=2000,s_month=6,s_day=1,e_year=2000,e_month=6,e_day=30,dataframe=df)\n",
    "#gen = convcnp.data_hydro.SawtoothGenerator(_\n",
    "\n",
    "#x_test = np.linspace(2000*365 + 30*6 + 1, 2000*365 + 30*6 + 30, 300)\n",
    "x_test = np.vstack([np.linspace(0,1,30),np.linspace(0,2,30)])\n",
    "#x_test = np.linspace(-2,2,300)\n",
    "#gp = stheno.GP(kernel)\n",
    "\n",
    "def plot_task(task, idx, legend):\n",
    "    #x_context, y_context = to_numpy(task['x_context'][idx]), to_numpy(task['y_context'][idx])\n",
    "    #x_target, y_target = to_numpy(task['x_target'][idx]), to_numpy(task['y_target'][idx])\n",
    "    x_context, y_context = to_numpy(task['x_context'][idx][:,0]), to_numpy(task['y_context'][idx])\n",
    "    x_target, y_target = to_numpy(task['x_target'][idx][:,0]), to_numpy(task['y_target'][idx])\n",
    "      \n",
    "    # Plot context and target sets.\n",
    "    plt.scatter(x_context, y_context, label='Context Set', color='black')\n",
    "    plt.scatter(x_target, y_target, label = 'Target Set', color='blue')\n",
    "    \n",
    "    # Infer GP posterior.\n",
    "    #post = gp  | (x_context, y_context)\n",
    "    \n",
    "    # Make and plot predictions on desired range.\n",
    "    #gp_mean, gp_lower, gp_upper = post(x_test).marginals()\n",
    "    #plt.plot(x_test, gp_mean, color='tab:green', label='Oracle GP')\n",
    "    #plt.fill_between(x_test, gp_lower, gp_upper, color='tab:green', alpha=0.1)\n",
    "    if legend:\n",
    "        plt.legend()\n",
    "\n",
    "task = gen.generate_task()\n",
    "\n",
    "fig = plt.figure(figsize=(24, 5))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plot_task(task, i, legend=i==2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iclRb0kP8uHe"
   },
   "source": [
    "In the above plots, the black points are the observed context set, and the green curve is the predictive function for the posterior Gaussian process (conditioned on the context set), with the _true_ GP kernel.\n",
    "\n",
    "Next, let's define the core building blocks for the ConvCNP model! \n",
    "\n",
    "# Encoder\n",
    "\n",
    "The encoder for our model will be a `ConvDeepSet` layer. This layer takes as input a set, and embeds it into a function space. The layer will also provide a discretization of the functional representation for the decoder. Further, for convenience, we add an additional, point-wise linear transformation to the functional representation to provide the decoder with the appropriate number of channels. In the module, this is called `self.g`, which is implemented as a simple linear layer, and applied after the discretization. \n",
    "\n",
    "In this implementation, we use a RBF kernel with a learned length-scale as the $\\psi$ function, which takes as input the distance between two input locations, and returns a scalar. Note that $\\psi$ can be any differentiable function of this form, e.g., implemented as an MLP. The method `self.rbf` implements $\\psi$ in this example. The helper function `compute_dists` computes the pair-wise distances between vectors of input locations.\n",
    "\n",
    "The forward method for this module accepts as inputs a context set -- $x$ and $y$ -- each of which is an order 3 tensor (batch size, num context points, `dim(x) / dim(y)`), and an additional order 3 tensor of locations ($x_{out}$) at which to evaluate the functional representation of the context set (this is the discretization for the decoder). It returns the values of the function at the locations $x_{out}$ for each of the tasks in the batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nc5YiX0R8uHf"
   },
   "outputs": [],
   "source": [
    "def compute_dists_1D(x, y):\n",
    "    \"\"\"Fast computation of pair-wise distances for the 1d case.\n",
    "\n",
    "    Args:\n",
    "        x (tensor): Inputs of shape (batch, n, 1).\n",
    "        y (tensor): Inputs of shape (batch, m, 1).\n",
    "\n",
    "    Returns:\n",
    "        tensor: Pair-wise distances of shape (batch, n, m).\n",
    "    \"\"\"\n",
    "    \n",
    "    return (x - y.permute(0, 2, 1)) ** 2\n",
    "\n",
    "\n",
    "def compute_dists(x_context, x_target):\n",
    "        '''\n",
    "        Compute dists for psi for 2D\n",
    "        '''\n",
    "        \n",
    "        t1 = (x_context[:, :, 0:1] - x_target.permute(0, 2, 1)[:, 0:1, :])**2\n",
    "        t2 = (x_context[:, :, 1:2] - x_target.permute(0, 2, 1)[:, 1:2, :])**2\n",
    "        \n",
    "        return (t1 + t2)\n",
    "\n",
    "class ConvDeepSet(nn.Module):\n",
    "    \"\"\"One-dimensional ConvDeepSet module. Uses an RBF kernel for psi(x, x').\n",
    "\n",
    "    Args:\n",
    "        out_channels (int): Number of output channels.\n",
    "        init_length_scale (float): Initial value for the length scale.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, out_channels, init_length_scale):\n",
    "        super(ConvDeepSet, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.in_channels = 2\n",
    "        self.g = self.build_weight_model()\n",
    "        self.sigma = nn.Parameter(np.log(init_length_scale) *\n",
    "                                  torch.ones(self.in_channels), requires_grad=True)\n",
    "        self.sigma_fn = torch.exp\n",
    "\n",
    "    def build_weight_model(self):\n",
    "        \"\"\"Returns a function point-wise function that transforms the\n",
    "        (in_channels + 1)-dimensional representation to dimensionality\n",
    "        out_channels.\n",
    "\n",
    "        Returns:\n",
    "            torch.nn.Module: Linear layer applied point-wise to channels.\n",
    "        \"\"\"\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(self.in_channels, self.out_channels),\n",
    "        )\n",
    "        init_sequential_weights(model)\n",
    "        return model\n",
    "    \n",
    "    def rbf(self, dists):\n",
    "        \"\"\"Compute the RBF values for the distances using the correct length\n",
    "        scales.\n",
    "\n",
    "        Args:\n",
    "            dists (tensor): Pair-wise distances between x and t.\n",
    "\n",
    "        Returns:\n",
    "            tensor: Evaluation of psi(x, t) with psi an RBF kernel.\n",
    "        \"\"\"\n",
    "        # Compute the RBF kernel, broadcasting appropriately.\n",
    "        scales = self.sigma_fn(self.sigma)[None, None, None, :]\n",
    "        a, b, c = dists.shape\n",
    "        return torch.exp(-0.5 * dists.view(a, b, c, -1) / scales ** 2)\n",
    "\n",
    "    def forward(self, x, y, t):\n",
    "        \"\"\"Forward pass through the layer with evaluations at locations t.\n",
    "\n",
    "        Args:\n",
    "            x (tensor): Inputs of observations of shape (n, 1).\n",
    "            y (tensor): Outputs of observations of shape (n, in_channels).\n",
    "            t (tensor): Inputs to evaluate function at of shape (m, 1).\n",
    "\n",
    "        Returns:\n",
    "            tensor: Outputs of evaluated function at z of shape\n",
    "                (m, out_channels).\n",
    "        \"\"\"\n",
    "        # Compute shapes.\n",
    "        batch_size = x.shape[0]\n",
    "        n_in = x.shape[1]\n",
    "        n_out = t.shape[1]\n",
    "        \n",
    "        pdb.set_trace()\n",
    "\n",
    "        # Compute the pairwise distances.\n",
    "        # Shape: (batch, n_in, n_out).\n",
    "        \n",
    "        #pdb.set_trace()\n",
    "        dists = compute_dists(x, t)\n",
    "\n",
    "        # Compute the weights.\n",
    "        # Shape: (batch, n_in, n_out, in_channels).\n",
    "        wt = self.rbf(dists)\n",
    "\n",
    "        # Compute the extra density channel.\n",
    "        # Shape: (batch, n_in, 1).\n",
    "        density = torch.ones(batch_size, n_in, 1).to(device)\n",
    "\n",
    "        # Concatenate the channel.\n",
    "        # Shape: (batch, n_in, in_channels + 1).\n",
    "        y_out = torch.cat([density, y], dim=2)\n",
    "\n",
    "        # Perform the weighting.\n",
    "        # Shape: (batch, n_in, n_out, in_channels + 1).\n",
    "        y_out = y_out.view(batch_size, n_in, -1, self.in_channels) * wt\n",
    "\n",
    "        # Sum over the inputs.\n",
    "        # Shape: (batch, n_out, in_channels + 1).\n",
    "        y_out = y_out.sum(1)\n",
    "\n",
    "        # Use density channel to normalize convolution.\n",
    "        density, conv = y_out[..., :1], y_out[..., 1:]\n",
    "        normalized_conv = conv / (density + 1e-8)\n",
    "        y_out = torch.cat((density, normalized_conv), dim=-1)\n",
    "\n",
    "        # Apply the point-wise function.\n",
    "        # Shape: (batch, n_out, out_channels).\n",
    "        y_out = y_out.view(batch_size * n_out, self.in_channels)\n",
    "        y_out = self.g(y_out)\n",
    "        y_out = y_out.view(batch_size, n_out, self.out_channels)\n",
    "\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0EOrgPB8uHk"
   },
   "source": [
    "## Decoder \n",
    "\n",
    "The decoder (denoted $\\rho$ in the paper) is any appropriate convolutional architecture. We provide two example architectures in the [convcnp.architectures](https://github.com/cambridge-mlg/convcnp/blob/master/convcnp/architectures.py) file that are used in the paper, but users should feel free to design and implement any architectures that seem appropriate. For example, in experiments not published, we have found that using depth-wise-separable convolutions can lead to performance gains as well as significant reductions in the number of model parameters. In this example, we will use the UNet architecture employed by ConvCNP XL in the paper, which can be imported from [convcnp.architectures](https://github.com/cambridge-mlg/convcnp/blob/master/convcnp/architectures.py). \n",
    "\n",
    "Finally, for the 1d case, we require an additional layer(s) on top of $\\rho$ to map back to the continuous setting (i.e., evaluate the output at any desired location). For this layer, we will use a layer that is similar to ConvDeepSet, involving another RBF function, $\\psi_{\\rho}$. However, it does not require any use of the density channel. Such a layer is constructed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nqPooof-8uHl"
   },
   "outputs": [],
   "source": [
    "class FinalLayer(nn.Module):\n",
    "    \"\"\"One-dimensional Set convolution layer. Uses an RBF kernel for psi(x, x').\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of inputs channels.\n",
    "        init_length_scale (float): Initial value for the length scale.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, init_length_scale):\n",
    "        super(FinalLayer, self).__init__()\n",
    "        self.out_channels = 1\n",
    "        self.in_channels = in_channels\n",
    "        self.g = self.build_weight_model()\n",
    "        self.sigma = nn.Parameter(np.log(init_length_scale) * torch.ones(self.in_channels), requires_grad=True)\n",
    "        self.sigma_fn = torch.exp\n",
    "\n",
    "    def build_weight_model(self):\n",
    "        \"\"\"Returns a function point-wise function that transforms the\n",
    "        (in_channels + 1)-dimensional representation to dimensionality\n",
    "        out_channels.\n",
    "\n",
    "        Returns:\n",
    "            torch.nn.Module: Linear layer applied point-wise to channels.\n",
    "        \"\"\"\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(self.in_channels, self.out_channels),\n",
    "        )\n",
    "        init_sequential_weights(model)\n",
    "        return model\n",
    "    \n",
    "    def rbf(self, dists):\n",
    "        \"\"\"Compute the RBF values for the distances using the correct length\n",
    "        scales.\n",
    "\n",
    "        Args:\n",
    "            dists (tensor): Pair-wise distances between x and t.\n",
    "\n",
    "        Returns:\n",
    "            tensor: Evaluation of psi(x, t) with psi an RBF kernel.\n",
    "        \"\"\"\n",
    "        # Compute the RBF kernel, broadcasting appropriately.\n",
    "        scales = self.sigma_fn(self.sigma)[None, None, None, :]\n",
    "        a, b, c = dists.shape\n",
    "        return torch.exp(-0.5 * dists.view(a, b, c, -1) / scales ** 2)\n",
    "\n",
    "    def forward(self, x, y, t):\n",
    "        \"\"\"Forward pass through the layer with evaluations at locations t.\n",
    "\n",
    "        Args:\n",
    "            x (tensor): Inputs of observations of shape (n, 1).\n",
    "            y (tensor): Outputs of observations of shape (n, in_channels).\n",
    "            t (tensor): Inputs to evaluate function at of shape (m, 1).\n",
    "\n",
    "        Returns:\n",
    "            tensor: Outputs of evaluated function at z of shape\n",
    "                (m, out_channels).\n",
    "        \"\"\"\n",
    "        # Compute shapes.\n",
    "        batch_size = x.shape[0]\n",
    "        n_in = x.shape[1]\n",
    "        n_out = t.shape[1]\n",
    "\n",
    "        # Compute the pairwise distances.\n",
    "        # Shape: (batch, n_in, n_out).\n",
    "        dists = compute_dists(x, t)\n",
    "\n",
    "        # Compute the weights.\n",
    "        # Shape: (batch, n_in, n_out, in_channels).\n",
    "        wt = self.rbf(dists)\n",
    "\n",
    "        # Perform the weighting.\n",
    "        # Shape: (batch, n_in, n_out, in_channels).\n",
    "        y_out = y.view(batch_size, n_in, -1, self.in_channels) * wt\n",
    "\n",
    "        # Sum over the inputs.\n",
    "        # Shape: (batch, n_out, in_channels).\n",
    "        y_out = y_out.sum(1)\n",
    "\n",
    "        # Apply the point-wise function.\n",
    "        # Shape: (batch, n_out, out_channels).\n",
    "        y_out = y_out.view(batch_size * n_out, self.in_channels)\n",
    "        y_out = self.g(y_out)\n",
    "        y_out = y_out.view(batch_size, n_out, self.out_channels)\n",
    "\n",
    "        return y_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RyiGN41J8uHr"
   },
   "source": [
    "\n",
    "## Complete Model\n",
    "\n",
    "We are now ready to construct the ConvCNP model! Upon initialization, the model takes two arguments: the convolutional architecture to use for $\\rho$, and the number of points (per unit interval) at which to evaluate the functional representation. The model will internally compute the exact discretization based on the context and target sets and this parameter, by spacing points evenly across an interval. The model also performs several other internal computations upon initialization, such as computing the intialization of the length scale for the RBF kernels in $\\psi$ and the output layer function $\\psi_{\\rho}$. \n",
    "\n",
    "In the forward pass, the model takes the context set (again represented as 3-tensors for $x$ and $y$), and the locations for the target set at which it should make predictions ($x_{out}$). It first determines the locations at which to evaluate the functional representation. Then, the encoder computes the functional representation and returns the values at the locations determined by the model. Finally, these locations are passed to $\\rho$, which produces a Gaussian prediction at each of the desired locations in the target set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w13T-E008uHs"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "class ConvCNP(nn.Module):\n",
    "    \"\"\"One-dimensional ConvCNP model.\n",
    "\n",
    "    Args:\n",
    "        learn_length_scale (bool): Learn the length scale.\n",
    "        points_per_unit (int): Number of points per unit interval on input.\n",
    "            Used to discretize function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rho, points_per_unit):\n",
    "        super(ConvCNP, self).__init__()\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.sigma_fn = nn.Softplus()\n",
    "        self.rho = rho\n",
    "        self.multiplier = 2 ** self.rho.num_halving_layers\n",
    "\n",
    "        # Compute initialisation.\n",
    "        self.points_per_unit = points_per_unit\n",
    "        init_length_scale = 2.0 / self.points_per_unit\n",
    "        \n",
    "        # Instantiate encoder\n",
    "        self.encoder = ConvDeepSet(out_channels=self.rho.in_channels,\n",
    "                                   init_length_scale=init_length_scale)\n",
    "        \n",
    "        # Instantiate mean and standard deviation layers\n",
    "        self.mean_layer = FinalLayer(in_channels=self.rho.out_channels,\n",
    "                                     init_length_scale=init_length_scale)\n",
    "        self.sigma_layer = FinalLayer(in_channels=self.rho.out_channels,\n",
    "                                      init_length_scale=init_length_scale)\n",
    "\n",
    "    def forward(self, x, y, x_out):\n",
    "        \"\"\"Run the model forward.\n",
    "\n",
    "        Args:\n",
    "            x (tensor): Observation locations of shape (batch, data, features).\n",
    "            y (tensor): Observation values of shape (batch, data, outputs).\n",
    "            x_out (tensor): Locations of outputs of shape (batch, data, features).\n",
    "            \n",
    "        Returns:\n",
    "            tuple[tensor]: Means and standard deviations of shape (batch_out, channels_out).\n",
    "        \"\"\"\n",
    "        # Determine the grid on which to evaluate functional representation.\n",
    "        x_min = min(torch.min(x).cpu().numpy(),\n",
    "                    torch.min(x_out).cpu().numpy(), -2.) - 0.1\n",
    "        x_max = max(torch.max(x).cpu().numpy(),\n",
    "                    torch.max(x_out).cpu().numpy(), 2.) + 0.1\n",
    "        num_points = int(to_multiple(self.points_per_unit * (x_max - x_min),\n",
    "                                     self.multiplier))\n",
    "        x_grid = torch.linspace(x_min, x_max, num_points).to(device)\n",
    "        x_grid = x_grid[None, :, None].repeat(x.shape[0], 1, 1)\n",
    "\n",
    "        # Apply first layer and conv net. Take care to put the axis ranging\n",
    "        # over the data last.\n",
    "        h = self.activation(self.encoder(x, y, x_grid))\n",
    "        h = h.permute(0, 2, 1)\n",
    "        h = h.reshape(h.shape[0], h.shape[1], num_points)\n",
    "        h = self.rho(h)\n",
    "        h = h.reshape(h.shape[0], h.shape[1], -1).permute(0, 2, 1)\n",
    "\n",
    "        # Check that shape is still fine!\n",
    "        if h.shape[1] != x_grid.shape[1]:\n",
    "            raise RuntimeError('Shape changed.')\n",
    "\n",
    "        # Produce means and standard deviations.\n",
    "        mean = self.mean_layer(x_grid, h, x_out)\n",
    "        sigma = self.sigma_fn(self.sigma_layer(x_grid, h, x_out))\n",
    "\n",
    "        return mean, sigma\n",
    "\n",
    "    @property\n",
    "    def num_params(self):\n",
    "        \"\"\"Number of parameters in model.\"\"\"\n",
    "        return np.sum([torch.tensor(param.shape).prod()\n",
    "                       for param in self.parameters()])\n",
    "    \n",
    "\n",
    "model = ConvCNP(rho=UNet(), points_per_unit=64)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zrxsiV9C8uHx"
   },
   "source": [
    "## Training Methods\n",
    "\n",
    "With our model in place, let's next define a training procedure. The `train` function below performs one training epoch (as defined by the number of tasks in an epoch from our data generator) as follows:\n",
    "1. Iterate over the tasks in the epoch\n",
    "2. For every task, condition on the context set, and make (Gaussian) predictions at the target locations.\n",
    "3. Compute the log-likelihood of the predictions under the target set.\n",
    "4. Backpropagate the error from the (negative) log-likelihood to the model parameters \n",
    "\n",
    "The `RunningAverage` average object will maintain a running average of the negative log-likelihood. In addition, we provide a plotting function so as to visually monitor the progress of the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QAaF-qaL8uHz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "def train(data, model, opt):\n",
    "    \"\"\"Perform a training epoch.\"\"\"\n",
    "    ravg = RunningAverage()\n",
    "    model.train()\n",
    "    for step, task in enumerate(data):\n",
    "        #pdb.set_trace()\n",
    "        y_mean, y_std = model(task['x_context'], task['y_context'], task['x_target'])\n",
    "        obj = -gaussian_logpdf(task['y_target'], y_mean, y_std, 'batched_mean')\n",
    "        obj.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        ravg.update(obj.item() / data.batch_size, data.batch_size)\n",
    "    return ravg.avg\n",
    "\n",
    "\n",
    "# Create a fixed set of outputs to predict at when plotting.\n",
    "#x_test = torch.linspace(-2., 2., 200)[None, :, None].to(device)\n",
    "x_test = torch.linspace(0., 1., 30)[None, :, None].to(device)\n",
    "\n",
    "\n",
    "def plot_model_task(model, task, idx, legend):\n",
    "    num_functions = task['x_context'].shape[0]\n",
    "    \n",
    "    # Make predictions with the model.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_mean, y_std = model(task['x_context'], task['y_context'], x_test.repeat(num_functions, 1, 1))\n",
    "    \n",
    "    # Plot the task and the model predictions.\n",
    "    x_context, y_context = to_numpy(task['x_context'][idx]), to_numpy(task['y_context'][idx])\n",
    "    x_target, y_target = to_numpy(task['x_target'][idx]), to_numpy(task['y_target'][idx])\n",
    "    y_mean, y_std = to_numpy(y_mean[idx]), to_numpy(y_std[idx])\n",
    "    \n",
    "    # Plot context and target sets.\n",
    "    plt.scatter(x_context, y_context, label='Context Set', color='black')\n",
    "    plt.scatter(x_target, y_target, label='Target Set', color='red')\n",
    "    \n",
    "    # Plot model predictions.\n",
    "    plt.plot(to_numpy(x_test[0]), y_mean, label='Model Output', color='blue')\n",
    "    plt.fill_between(to_numpy(x_test[0]),\n",
    "                     y_mean + 2 * y_std,\n",
    "                     y_mean - 2 * y_std,\n",
    "                     color='tab:blue', alpha=0.2)\n",
    "    if legend:\n",
    "        plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aH2roeSh8uH3"
   },
   "source": [
    "\n",
    "## Training loop for the model\n",
    "\n",
    "Now we can write a standard PyTorch optimization loop. First, we instantiate an optimizer (here we use an Adam optimizer). Then, we simply iterate for as many epochs as necessary to train the model. Every so often we will print the training error (since the data are generated on the fly, this is also a reasonable estimator for the held out error), and produce a plot demonstrating the model performance. Note that the next block of code will train the model for 100 epochs. This may take some time, and it is highly recommended to do so with a GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zir1KrWo8uH4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-271369785475>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Initialize optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Run the training loop.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Some training hyper-parameters:\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 100\n",
    "PLOT_FREQ = 10\n",
    "\n",
    "# Initialize optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Run the training loop.\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    #print ('MGM Epoch %s: NLL %.3f' % (epoch, train_obj))  \n",
    "    # Compute training objective.\n",
    "    train_obj = train(gen, model, opt)\n",
    "\n",
    "    # Plot model behaviour every now and again.\n",
    "    if epoch % PLOT_FREQ == 0:\n",
    "        print('Epoch %s: NLL %.3f' % (epoch, train_obj))\n",
    "        task = gen.generate_task()\n",
    "        fig = plt.figure(figsize=(24, 5))\n",
    "        for i in range(3):\n",
    "            plt.subplot(1, 3, i + 1)\n",
    "            plot_model_task(model, task, idx=i, legend=i==2)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('Epoch %s: NLL %.3f' % (epoch, train_obj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q6e4fb7J8uH9"
   },
   "source": [
    "## Evaluation \n",
    "\n",
    "We can also evaluate the trained model to get better estimates of its performance. To do so, we will first introduce an additional data generator that will generate far more tasks for us. Then, we will loop over many tasks, and compute (and print) the average log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tW8EadNR8uH-"
   },
   "outputs": [],
   "source": [
    "# Instantiate data generator for testing.\n",
    "NUM_TEST_TASKS = 2048\n",
    "gen_test = convcnp.data.GPGenerator(kernel=kernel, num_tasks=NUM_TEST_TASKS)\n",
    "\n",
    "# Compute average task log-likelihood.\n",
    "ravg = RunningAverage()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for step, task in enumerate(gen_test):\n",
    "        y_mean, y_std = model(task['x_context'], task['y_context'], task['x_target'])\n",
    "        obj = -gaussian_logpdf(task['y_target'], y_mean, y_std, 'batched_mean')\n",
    "        ravg.update(obj.item() / gen_test.batch_size, gen_test.batch_size)\n",
    "\n",
    "print('Model averages a log likelihood of %.2f on unseen tasks.' % -ravg.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "convcnp_regression_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
